{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# StarAI - Get data and Preprocess\n",
    "\n",
    "This notebook summarizes the preprocessing of the StarAI datasets. Cf. https://github.com/UCLA-StarAI/Density-Estimation-Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Black Codeformatter\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = 4  # Cores on your current CPU, will speed up the process.\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elki_interface\n",
    "\n",
    "from elki_interface.exps import (\n",
    "    starai_original_filepath,\n",
    "    dataset_filepath,\n",
    "    get_starai_dataset_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_starai_dfs(name=\"nltcs\"):\n",
    "    # Load\n",
    "    fp_train = starai_original_filepath(name=name, kind=\"train\")\n",
    "    fp_test = starai_original_filepath(name=name, kind=\"test\")\n",
    "\n",
    "    df_train = pd.read_csv(fp_train, header=None)\n",
    "    df_test = pd.read_csv(fp_test, header=None)\n",
    "\n",
    "    # If test is larger than train, we swap them!\n",
    "    n_rows_train = df_train.shape[0]\n",
    "    n_rows_test = df_test.shape[0]\n",
    "    if n_rows_test > n_rows_train:\n",
    "        warnings.warn(\n",
    "            \"Test set larger than training set. We assume this is a mistake and we swap them.\"\n",
    "        )\n",
    "        df_temp = df_test\n",
    "        df_test = df_train\n",
    "        df_train = df_temp\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def drop_constant_columns(df_train, df_test):\n",
    "    constant_columns = _detect_constant_columns([df_train, df_test])\n",
    "    df_train, df_test = (\n",
    "        df_train.drop(constant_columns, axis=1),\n",
    "        df_test.drop(constant_columns, axis=1),\n",
    "    )\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def _detect_constant_columns(dfs, constant_column_uvalues=1):\n",
    "    \"\"\"Ugly, but works\"\"\"\n",
    "    try:\n",
    "        result = []\n",
    "        for df in iter(dfs):\n",
    "            result += _detect_constant_columns(df)\n",
    "        return result\n",
    "    except TypeError:\n",
    "        # We assume a single dataframe was passed\n",
    "        return [col for col in dfs if dfs[col].nunique() <= constant_column_uvalues]\n",
    "\n",
    "\n",
    "def add_headers(df_train, df_test):\n",
    "    pxs_headers = _headers_pxs(df_train)\n",
    "    assert pxs_headers == _headers_pxs(\n",
    "        df_test\n",
    "    ), \"Headers for train and test set differ. That is not supposed to happen!\"\n",
    "\n",
    "    df_train.columns, df_test.columns = pxs_headers, pxs_headers\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def _headers_pxs(df):\n",
    "    return [\"att_{}\".format(x) for x in range(len(df.columns))]\n",
    "\n",
    "\n",
    "def save_starai_dfs(df_train, df_test, name=\"nltcs\", step=1):\n",
    "    fp_train = dataset_filepath(name=name, step=step, kind=\"train\", check=False)\n",
    "    fp_test = dataset_filepath(name=name, step=step, kind=\"test\", check=False)\n",
    "\n",
    "    df_train.to_csv(fp_train, index=False)\n",
    "    df_test.to_csv(fp_test, index=False)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workfow_raw_to_s01(name, verbose=True):\n",
    "    if verbose:\n",
    "        msg = \"Start on dataset {}\".format(name)\n",
    "        print(msg)\n",
    "\n",
    "    df_train, df_test = load_raw_starai_dfs(name=name)\n",
    "    df_train, df_test = drop_constant_columns(df_train, df_test)\n",
    "    df_train, df_test = add_headers(df_train, df_test)\n",
    "\n",
    "    if verbose:\n",
    "        msg = \"Done with dataset {}\".format(name)\n",
    "        print(msg)\n",
    "\n",
    "    return save_starai_dfs(df_train, df_test, name=name, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '../../data/raw/datasets-starai'...\n",
      "Checking out files: 100% (100/100), done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf ../../data/raw/datasets-starai\n",
    "git clone git@github.com:UCLA-StarAI/Density-Estimation-Datasets.git ../../data/raw/datasets-starai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: checking out '44c51c50c43686c889de21dcd68cb80820abc9b9'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by performing another checkout.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -b with the checkout command again. Example:\n",
      "\n",
      "  git checkout -b <new-branch-name>\n",
      "\n",
      "HEAD is now at 44c51c5 Merge pull request #1 from arranger1044/master\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../../data/raw/datasets-starai\n",
    "git checkout 44c51c50c43686c889de21dcd68cb80820abc9b9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess: Raw -> Step 01\n",
    "\n",
    "This is currently the only step we need. If in the future more preprocessing is necessary, this can simply be added as the next step. In that way, the layout and the logic is flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zissou/repos/missmercs/data/raw/datasets-starai/datasets/nltcs/nltcs.train.data\n"
     ]
    }
   ],
   "source": [
    "nltcs_filepath = starai_original_filepath(name=\"nltcs\", kind=\"train\")\n",
    "print(nltcs_filepath)\n",
    "assert nltcs_filepath.exists(), \"This path ({}) does not exist\".format(nltcs_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start on dataset nltcs\n",
      "Done with dataset nltcs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workfow_raw_to_s01(name=\"nltcs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all datasets\n",
    "\n",
    "Repeat the procedure for all datasets at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accidents',\n",
       " 'ad',\n",
       " 'adult',\n",
       " 'baudio',\n",
       " 'bbc',\n",
       " 'bnetflix',\n",
       " 'book',\n",
       " 'c20ng',\n",
       " 'connect4',\n",
       " 'cr52',\n",
       " 'cwebkb',\n",
       " 'dna',\n",
       " 'jester',\n",
       " 'kdd',\n",
       " 'kosarek',\n",
       " 'moviereview',\n",
       " 'msnbc',\n",
       " 'msweb',\n",
       " 'mushrooms',\n",
       " 'nips',\n",
       " 'nltcs',\n",
       " 'ocr_letters',\n",
       " 'plants',\n",
       " 'pumsb_star',\n",
       " 'rcv1',\n",
       " 'tmovie',\n",
       " 'tretail',\n",
       " 'voting']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starai_dataset_names = get_starai_dataset_names()\n",
    "starai_dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=4)]: Done   2 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=4)]: Done   3 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=4)]: Done   4 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=4)]: Done   6 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=4)]: Done   7 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=4)]: Done   8 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=4)]: Done   9 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=4)]: Done  11 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=4)]: Done  12 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=4)]: Done  13 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=4)]: Done  14 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=4)]: Done  15 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=4)]: Done  16 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=4)]: Done  18 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=4)]: Done  19 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=4)]: Done  20 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=4)]: Done  21 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=4)]: Done  22 out of  28 | elapsed:   13.2s remaining:    3.6s\n",
      "[Parallel(n_jobs=4)]: Done  23 out of  28 | elapsed:   13.9s remaining:    3.0s\n",
      "[Parallel(n_jobs=4)]: Done  24 out of  28 | elapsed:   14.3s remaining:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  28 | elapsed:   14.4s remaining:    1.7s\n",
      "[Parallel(n_jobs=4)]: Done  26 out of  28 | elapsed:   15.3s remaining:    1.2s\n",
      "[Parallel(n_jobs=4)]: Done  28 out of  28 | elapsed:   19.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  28 out of  28 | elapsed:   19.0s finished\n",
      "Preprocessing StarAI done\n"
     ]
    }
   ],
   "source": [
    "Parallel(n_jobs=N_JOBS, verbose=51)(delayed(workfow_raw_to_s01)(ds) for ds in starai_dataset_names)\n",
    "\n",
    "print(\"Preprocessing StarAI done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lgtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "missmercs",
   "language": "python",
   "name": "missmercs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
