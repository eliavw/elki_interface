{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# StarAI - Queries\n",
    "\n",
    "Generating queries for the `STARAI` datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Black Codeformatter\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "N_JOBS = 4  # Cores on your current CPU, will speed up the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import fileinput\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mercs\n",
    "from mercs.utils.encoding import query_to_code, code_to_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elki_interface\n",
    "\n",
    "from elki_interface.exps import (\n",
    "    query_filepath,\n",
    "    dataset_filepath,\n",
    "    get_starai_dataset_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def generate_query(nb_atts, targ_idx=-1, nb_qry=10, random_state=42):\n",
    "    # init ids\n",
    "    attr_ids = list(range(nb_atts))\n",
    "    targ_ids = [attr_ids[targ_idx]]  # Last attribute by default\n",
    "    desc_ids = [e for e in attr_ids if e not in targ_ids]\n",
    "    miss_ids = []\n",
    "\n",
    "    q_targ = [targ_ids]\n",
    "    q_desc = [desc_ids]\n",
    "    q_miss = [miss_ids]\n",
    "\n",
    "    # Start query buiding\n",
    "    nb_of_attributes_to_make_missing = np.linspace(\n",
    "        0, nb_atts - 1, nb_qry, endpoint=False, dtype=int\n",
    "    )\n",
    "    nb_items_to_transfer = np.ediff1d(nb_of_attributes_to_make_missing)\n",
    "\n",
    "    for qry_id, e in enumerate(nb_items_to_transfer):\n",
    "        desc_ids, miss_ids = _transfer_contents(\n",
    "            desc_ids, miss_ids, nb_items_to_transfer=e, random_state=random_state\n",
    "        )\n",
    "\n",
    "        # print(desc_ids, miss_ids, targ_ids)\n",
    "        q_targ.append(targ_ids)\n",
    "        q_desc.append(desc_ids)\n",
    "        q_miss.append(miss_ids)\n",
    "\n",
    "    return q_desc, q_targ, q_miss\n",
    "\n",
    "\n",
    "def _transfer_contents(list_one, list_two, nb_items_to_transfer=1, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    list_one, list_two = list_one.copy(), list_two.copy()\n",
    "\n",
    "    idx_to_transfer = np.random.choice(\n",
    "        range(len(list_one)), nb_items_to_transfer, replace=False\n",
    "    )\n",
    "    content_to_transfer = [\n",
    "        e for idx, e in enumerate(list_one) if idx in idx_to_transfer\n",
    "    ]\n",
    "\n",
    "    for e in content_to_transfer:\n",
    "        list_one.remove(e)\n",
    "        list_two.append(e)\n",
    "\n",
    "    return list_one, list_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def generate_queries(dataset, max_nb_queries=10, random_state=42, nb_iterations=10):\n",
    "    q_codes = []\n",
    "\n",
    "    # Derive Parameters\n",
    "    fn = dataset_filepath(dataset, step=1, kind=\"test\", extension=\"csv\")\n",
    "    df = pd.read_csv(fn, header=None, index_col=None)\n",
    "\n",
    "    nb_atts = len(df.columns)\n",
    "    nb_qry = min(nb_atts - 1, max_nb_queries)\n",
    "\n",
    "    targ_ids = np.random.choice(nb_atts, nb_iterations, replace=True)\n",
    "\n",
    "    for i, target_idx in enumerate(targ_ids):\n",
    "        # Generate queries\n",
    "        q_desc, q_targ, q_miss = generate_query(\n",
    "            nb_atts, targ_idx=target_idx, nb_qry=nb_qry, random_state=random_state + i\n",
    "        )\n",
    "\n",
    "        for q_idx in range(nb_qry):\n",
    "            q_codes.append(query_to_code(q_desc[q_idx], q_targ[q_idx], q_miss[q_idx]))\n",
    "\n",
    "    q_codes = np.r_[q_codes]  # Convert to proper np.ndarray\n",
    "\n",
    "    # Save\n",
    "    fn_qry = query_filepath(dataset, keyword=\"default\")\n",
    "    np.save(fn_qry, q_codes)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['accidents', 'ad', 'adult', 'baudio', 'bbc', 'bnetflix', 'book', 'c20ng', 'connect4', 'cr52', 'cwebkb', 'dna', 'jester', 'kdd', 'kosarek', 'moviereview', 'msnbc', 'msweb', 'mushrooms', 'nips', 'nltcs', 'ocr_letters', 'plants', 'pumsb_star', 'rcv1', 'tmovie', 'tretail', 'voting']"
      ],
      "text/plain": [
       "['accidents',\n",
       " 'ad',\n",
       " 'adult',\n",
       " 'baudio',\n",
       " 'bbc',\n",
       " 'bnetflix',\n",
       " 'book',\n",
       " 'c20ng',\n",
       " 'connect4',\n",
       " 'cr52',\n",
       " 'cwebkb',\n",
       " 'dna',\n",
       " 'jester',\n",
       " 'kdd',\n",
       " 'kosarek',\n",
       " 'moviereview',\n",
       " 'msnbc',\n",
       " 'msweb',\n",
       " 'mushrooms',\n",
       " 'nips',\n",
       " 'nltcs',\n",
       " 'ocr_letters',\n",
       " 'plants',\n",
       " 'pumsb_star',\n",
       " 'rcv1',\n",
       " 'tmovie',\n",
       " 'tretail',\n",
       " 'voting']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starai_dataset_names = get_starai_dataset_names()\n",
    "starai_dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done   2 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done   3 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done   4 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=4)]: Done   6 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done   7 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=4)]: Done   8 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=4)]: Done   9 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=4)]: Done  11 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=4)]: Done  12 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=4)]: Done  13 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=4)]: Done  14 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=4)]: Done  15 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=4)]: Done  16 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=4)]: Done  18 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=4)]: Done  19 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=4)]: Done  20 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=4)]: Done  21 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=4)]: Done  22 out of  28 | elapsed:    4.8s remaining:    1.3s\n",
      "[Parallel(n_jobs=4)]: Done  23 out of  28 | elapsed:    4.9s remaining:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done  24 out of  28 | elapsed:    5.3s remaining:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  28 | elapsed:    5.5s remaining:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  26 out of  28 | elapsed:    5.6s remaining:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done  28 out of  28 | elapsed:    6.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  28 out of  28 | elapsed:    6.9s finished\n",
      "\n",
      "\n",
      "Making STARAI queries done.\n"
     ]
    }
   ],
   "source": [
    "Parallel(n_jobs=N_JOBS, verbose=51)(delayed(generate_queries)(ds, random_state=RANDOM_STATE) for ds in starai_dataset_names)\n",
    "\n",
    "print(\"\"\"\n",
    "\n",
    "Making STARAI queries done.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lgtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "missmercs",
   "language": "python",
   "name": "missmercs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
